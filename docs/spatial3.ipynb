{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Jupyter Notebook](https://img.shields.io/badge/Source%20on%20GitHub-orange)](https://github.com/laminlabs/lamin-usecases/blob/main/docs/spatial2.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a spatial ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we show how we can query, access, and combine several SpatialData datasets across different technologies to train a Dense Net which predicts cell types Xenium data from an associated H&E image.\n",
    "Specifically, we use the H&E image from Visium data, and the cell type information from overlapping Xenium data.\n",
    "Both modalities are spatially aligned via an affine transformation.\n",
    "\n",
    "This tutorial is adapted from the [SpatialData documentation](https://spatialdata.scverse.org/en/stable/tutorials/notebooks/notebooks/examples/densenet.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import lamindb as ln\n",
    "import numpy as np\n",
    "\n",
    "import spatialdata as sd\n",
    "from spatialdata import transform\n",
    "from spatialdata.dataloader.datasets import ImageTilesDataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "ln.track()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we query for Visium and Xenium datasets and create a merged dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenium_1_sd = ln.Artifact.filter(key=\"xenium_aligned_1_guide_min.zarr\").one().load()\n",
    "visium_sd = ln.Artifact.filter(key=\"visium_aligned_guide_min.zarr\").one().load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sd = sd.SpatialData(\n",
    "    images={\n",
    "        \"CytAssist_FFPE_Human_Breast_Cancer_full_image\": visium_sd.images[\n",
    "            \"CytAssist_FFPE_Human_Breast_Cancer_full_image\"\n",
    "        ],\n",
    "    },\n",
    "    shapes={\n",
    "        \"cell_circles\": xenium_1_sd.shapes[\"cell_circles\"],\n",
    "        \"cell_boundaries\": xenium_1_sd.shapes[\"cell_boundaries\"],\n",
    "    },\n",
    "    tables={\"table\": xenium_1_sd[\"table\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Visium image is rotated with respect to the Xenium data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://spatialdata.scverse.org/en/stable/_images/dense_net_cell_types.png\" width=\"600\" height=\"500\" alt=\"Dense network of cell types\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an `ImageTilesDataset` using our merged `SpatialData` object.\n",
    "We further import an image tile transform, the corresponding Pytorch Lightning `DataModule`, and the final `DenseNet` model from an existing script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{dropdown} Code of tile_transform, ImageTilesDataset and the DenseNetModel\n",
    "```{eval-rst}\n",
    ".. literalinclude:: spatial_ml.py\n",
    "   :language: python\n",
    "   :caption: Spatial cell type classification model definition\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from spatial_ml import tile_transform, TilesDataModule, DenseNetModel\n",
    "\n",
    "dataset = ImageTilesDataset(\n",
    "    sdata=merged_sd,\n",
    "    regions_to_images={\"cell_circles\": \"CytAssist_FFPE_Human_Breast_Cancer_full_image\"},\n",
    "    regions_to_coordinate_systems={\"cell_circles\": \"aligned\"},\n",
    "    table_name=\"table\",\n",
    "    tile_dim_in_units=6\n",
    "    * np.mean(\n",
    "        transform(merged_sd[\"cell_circles\"], to_coordinate_system=\"aligned\").radius\n",
    "    ),\n",
    "    transform=tile_transform,\n",
    "    rasterize=True,\n",
    "    rasterize_kwargs={\"target_width\": 32},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we only need to set up a DataModule, our model, and we can start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "pl.seed_everything(7)\n",
    "\n",
    "tiles_data_module = TilesDataModule(batch_size=64, num_workers=8, dataset=dataset)\n",
    "\n",
    "tiles_data_module.setup()\n",
    "train_dl = tiles_data_module.train_dataloader()\n",
    "val_dl = tiles_data_module.val_dataloader()\n",
    "test_dl = tiles_data_module.test_dataloader()\n",
    "\n",
    "model = DenseNetModel(\n",
    "    learning_rate=1e-5,\n",
    "    in_channels=dataset[0][0].shape[0],\n",
    "    num_classes=len(merged_sd[\"table\"].obs[\"celltype_major\"].cat.categories.tolist()),\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    callbacks=[\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "        TQDMProgressBar(refresh_rate=5),\n",
    "    ],\n",
    "    log_every_n_steps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule=tiles_data_module)\n",
    "trainer.test(model, datamodule=tiles_data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to perform a prediction and evaluate it like outlined in the [original guide](https://spatialdata.scverse.org/en/stable/tutorials/notebooks/notebooks/examples/densenet.html), we would see predictions like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "source": [
    "<img src=\"https://spatialdata.scverse.org/en/stable/_images/dense_net_predicted.png\" width=\"1000\" height=\"450\" alt=\"Model predictions\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamindb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbproject": {
   "dependency": null,
   "id": "daeFs3PkquDW",
   "time_init": "2022-07-12T17:54:02.478333+00:00",
   "version": "draft"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
